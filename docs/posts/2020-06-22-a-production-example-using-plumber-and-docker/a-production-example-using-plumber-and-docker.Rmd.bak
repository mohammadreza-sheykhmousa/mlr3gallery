---
title: "A production example using plumber and Docker"
categories:
  - mlr3pipelines
  - production
author:
  - name: Lennart Schneider
date: 06-22-2020
description: |
  We write a REST API using plumber and deploy it using Docker.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
library(mlr3book)
```

FIXME: Some fancy intro words... Give credit to some docs and blog posts where it is due.


### Modeling Background

```{r}
library(data.table)
library(mlr3)
library(mlr3pipelines)
```

Lets create a toy pipeline that we want to use to predict the `Species` of the well known `r ref("mlr_tasks_iris", "iris")` `r ref("Task", "Task")`:

```{r}
task = tsk("iris")
```

Regarding modeling, we will keep it very simple and use the `r ref("mlr_learners_classif.rpart", "rpart learner")`. Missing numerical features will be imputed bu their median via `r ref("PipeOpImputeMedian", "PipeOpImputeMedian")`.

```{r}
g = po("imputemedian") %>>% lrn("classif.rpart")
```

We wrap this `r ref("Graph")` in a `r ref("GraphLearner")` and can train on the `Task`:

```{r}
gl = GraphLearner$new(g)
gl$train(task)
```

We can inspect the trained pipeline looking at:

```{r, eval = FALSE}
gl$model
```

Furthermore, we can save the trained pipeline, i.e., as `"gl.rds"`:

```{r}
saveRDS(gl, "gl.rds")
```

Putting everything in a file, `train_gl.R` looks like the following, which we can then source before moving on:

```{r, train_gl}
# train_gl.R

library(mlr3)
library(mlr3pipelines)

task = tsk("iris")

g = po("imputemedian") %>>% lrn("classif.rpart")

gl = GraphLearner$new(g)

gl$train(task)

saveRDS(gl, "gl.rds")
```

Our goal of our REST (representational state transfer) API (application programming interface) will be to predict the `Species` of a new observation, i.e., it should do something like the following:

```{r}
gl$predict_newdata(data.table(Petal.Length = NA, Petal.Width = 3.14, Sepal.Length = 6.96, Sepal.Width = 0.12))
```

However, in our REST API, the `newdata` will be received at an endpoint that accepts a particular input. In the next section we will use `plumber` to set up our web service.

### Using plumber to set up our REST API

The package `r cran_pkg("plumber")` allows us to create a REST API by simply commenting existing R code. `plumber` makes use of these comments to define the web service. Running `plumber::plumb` on the commented R file then results in a runnable web service that other systems can interact with over a network.

As an endpoint for predicting the `Species`, we will use a `POST` request. This will allow us to enclose data in the body of the request message. More precisely, we assume that the data will be provided in the JSON format.

When a `POST` request containing the data (in JSON format) is received our code must then:

1. convert the input (in JSON format) to a `data.table` with all four feature columns (`Petal.Length`, `Petal.Width`, `Sepal.Length`, `Sepal.Width`) being numeric

2. predict the `Species` based on the input using our trained pipeline and provide an output that can be understood by the client

The following R file, `predict_gl.R` loads our trained pipepline and provides an endpoint for a `POST` request, `"/predict_species"`. The incoming data then is converted using `jsonlite::fromJSON`. We expect the incoming data to either be JSON objects in an array or nested JSON objects and therefore we bind the converted vectors rowwise to a `data.table` using  `data.table::rbindlist`. We then convert all columns to numeric values and can finally predict the `Species` using our trained pipeline. As no default serialization from `R6` objects to JSON objects exists (yet), we wrap the `r ref("Prediction", "Prediction")` in a `data.table`:

```{r, predict_gl, eval = FALSE}
# predict_gl.R

library(data.table)
library(jsonlite)
library(mlr3)
library(mlr3pipelines)

gl = readRDS("gl.rds")

#* @post /predict_species
function(req) {
  # get the JSON string from the post body
  newdata = fromJSON(req$postBody, simplifyVector = FALSE)
  # expect either JSON objects in an array or nested JSON objects
  newdata = rbindlist(newdata)
  # convert all columns to numeric
  newdata = newdata[, lapply(.SD, FUN = as.numeric)]
  # predict and return as a data.table
  as.data.table(gl$predict_newdata(newdata))
}
```

Note that the only difference to a regular R file is the comment

```{r, eval = FALSE}
#* @post /predict_species`
```

telling `plumber` to construct the endpoint `"/predict_species"` for a `POST` request.

We can then run `plumber::plumb`. The following code sets up the web service locally on your personal machine at port 1030 (we use such a high number because some systems require administrator rights to allow processes to listen to lower ports):

```{r, eval = FALSE}
library(plumber)
r = plumb("predict_gl.R")
r$run(port = 1030, host = "0.0.0.0")
```

Congratulations, your first REST API is running on your local machine. We can test it by providing some data, using `curl` via the command line:

```{}
curl --data '[{"Petal.Length":1.34, "Petal.Width":2, "Sepal.Length":6.1, "Sepal.Width": 4.31}]' "http://127.0.0.1:1030/predict_species"
```

This should return the predicted `Species`:

```{}
[{"row_id":1,"response":"setosa"}]
```

Alternatively, we can also use the `httr::POST` function within R:

```{r, eval = FALSE}
newdata = '[{"Petal.Length":1.34, "Petal.Width":2, "Sepal.Length":6.1, "Sepal.Width":4.31}]'
resp = httr::POST(url = "http://127.0.0.1:1030/predict_species", body = newdata, encode = "json")
httr::content(resp)
```

We can further play around a bit more and provide more than a single new observation and also check whether our missing value imputation works:

```{r, eval = FALSE}
newdata = '[{"Petal.Length":"abc", "Petal.Width":3.14, "Sepal.Length":6.96, "Sepal.Width":0.12},
  {"Petal.Length":3.41, "Petal.Width":"def", "Sepal.Length":"ghi", "Sepal.Width":7.3}]'
resp = httr::POST(url = "http://127.0.0.1:1030/predict_species", body = newdata, encode = "json")
httr::content(resp)
```

In the following final section we want to use `Docker` to run a virtual machine as a container (an instance of a snapshot of a machine at a moment in time).

### Use Docker to Deploy our REST API

A `Docker` container image is a lightweight, standalone, executable package of software that includes everything needed to run an application. Suppose we want to run our REST API on an Amazon Web Service or Microsoft's Azure cloud. Then we can use a `Docker` container to easily set up our web service without going through the hassle of configuring manually our hosting instance.

We are going to need two things: An image and a container. An image defines the OS and software while the container is the actual running instance of the image. To build a `Docker` image we have to specify a `Dockerfile`. Note that it is sensible to set up the whole project in its own directory, e.g., `~/mlr3_api`.

Every `Dockerfile` starts with a `FROM` statement describing the image we are building our image from. In our case we want an R based image that ideally already has `plumber` and its dependencies installed. Luckily, the `trestletech/plumber` image exists:

```{}
FROM trestletech/plumber
```

We then install the R packages needed to set up our REST API (note that we can skip `jsonlite`, because `plumber` already depends on it):

```{}
RUN R -e 'install.packages(c("data.table", "mlr3", "mlr3pipelines"))'
```

Next, we copy our trained pipeline (`gl.rds`) and our R file to predict (`predict_gl.R`) to a new directory `/data` and set this as the working directory:

```{}
RUN mkdir /data
COPY gl.rds /data
COPY predict_gl.R /data
WORKDIR /data
```

Finally, we listen on port 1030 and start the server (this is analogously done as manually calling `plumber::plumb` on the `predict_gl.R` file and running it):

```{}
EXPOSE 1030
ENTRYPOINT ["R", "-e", \
    "r = plumber::plumb('/data/predict_gl.R'); r$run(port = 1030, host = '0.0.0.0')"]
```

The complete `Dockerfile` looks like the following:

```{}
# Dockerfile

FROM trestletech/plumber

RUN R -e 'install.packages(c("data.table", "mlr3", "mlr3pipelines"))'

RUN mkdir /data
COPY gl.rds /data
COPY predict_gl.R /data
WORKDIR /data

EXPOSE 1030
ENTRYPOINT ["R", "-e", \
    "r = plumber::plumb('/data/predict_gl.R'); r$run(port = 1030, host = '0.0.0.0')"]
```

To build the image we open a terminal in the `mlr3_api` directory and run:

```{}
docker build -t mlr3-plumber-demo .
```

This may take quite some time.

To finally run the container, simply use:

```{}
docker run --rm -p 1030:1030 mlr3-plumber-demo
```

You can then proceed to provide some data via `curl` or `httr::POST` (to the same local address, because the `Docker` container is still running on your local machine).

To stop all running containers use:

```{}
docker stop $(docker ps -a -q)
```

Finally, you can proceed to deploy your container to an Amazon Web Service or an Azure Cloud. For the latter, the package `r cran_pkg("AzureContainers")` is especially helpful.
